```
ğŸ“Š Multi-Scale TimeXer Architecture Diagram

Input Time Series: [B, seq_len=96, n_vars]
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Multi-Scale Patching            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Patch_8   â”‚ Patch_16  â”‚ Patch_24  â”‚
    â”‚ 12 tokens â”‚ 6 tokens  â”‚ 4 tokens  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Independent Embedding           â”‚
        â”‚  patch_size â†’ d_model (256)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚[B,V,13,D] â”‚[B,V,7,D]  â”‚[B,V,5,D]  â”‚
    â”‚+global tokâ”‚+global tokâ”‚+global tokâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Fusion Module                â”‚
        â”‚   (Attention/Gated/Scale-Aware)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            [B, n_vars, 25, d_model]
            (13+7+5=25 total tokens)
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Reshape for Encoder             â”‚
        â”‚   [B*n_vars, 25, d_model]           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Transformer Encoder            â”‚
        â”‚    (Self-Attention + Cross-Attn)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Prediction Head               â”‚
        â”‚   [B, n_vars, pred_len]             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” **é—œéµå¯¦ç¾ç´°ç¯€å›ç­”**

### **1. æ˜¯å¦åœ¨ Embedding å‰é‚„æ˜¯å¾Œï¼Ÿ**
**ç­”æ¡ˆï¼šåœ¨ Embedding å…§éƒ¨ï¼**

å…·é«”ä¾†èªªï¼š
- æ¯å€‹å°ºåº¦**å…ˆç¨ç«‹åš embedding**
- ç„¶å¾Œåœ¨ embedding çš„æœ€å¾Œéšæ®µåš **fusion**
- Fusion çš„çµæœæ‰é€å…¥ Transformer Encoder

### **2. æ˜¯å¦æœ¬èº«å°±æ˜¯ embeddingï¼Ÿ**
**ç­”æ¡ˆï¼šFusion æ˜¯ Embedding çš„ä¸€éƒ¨åˆ†ï¼**

æˆ‘å€‘çš„ `MultiScaleEnEmbedding` é¡åŒ…å«ï¼š
1. **Multi-scale patching**
2. **Per-scale linear embedding**
3. **Position embedding**
4. **Scale-specific global tokens**
5. **ğŸ”¥ Fusion module** â† é€™æ˜¯é—œéµå‰µæ–°
6. **æœ€çµ‚è¼¸å‡º dropout**

### **3. æœ€çµ‚æ˜¯å¦ concat æˆå¤§åºåˆ—ï¼Ÿ**
**ç­”æ¡ˆï¼šæ˜¯çš„ï¼**

ä¸ç®¡ç”¨å“ªç¨® fusion æ–¹æ³•ï¼Œæœ€çµ‚éƒ½æœƒï¼š
```python
# æ‰€æœ‰ fusion æ–¹æ³•çš„å…±åŒé»
final_tokens = fusion_result  # [B, n_vars, total_patches, d_model]

# é‡å¡‘ç‚º encoder è¼¸å…¥æ ¼å¼
encoder_input = final_tokens.reshape(B * n_vars, total_patches, d_model)
```

é€™å€‹ `total_patches = 25` çš„åºåˆ—å°±æ˜¯é€å…¥ Transformer çš„æœ€çµ‚ token åºåˆ—ã€‚

### **4. èˆ‡åŸå§‹æ¶æ§‹çš„å·®ç•°**

| åŸå§‹ TimeXer | æˆ‘å€‘çš„ Multi-Scale TimeXer |
|-------------|---------------------------|
| å–®ä¸€ patch size (16) | å¤šå€‹ patch sizes [8,16,24] |
| 6 patches + 1 global = 7 tokens | 22 patches + 3 globals = 25 tokens |
| ç°¡å–® linear embedding | è¤‡é›œ fusion embedding |
| å›ºå®šæ„Ÿå—é‡ | å¤šå°ºåº¦æ„Ÿå—é‡ |
