
# üìò TimeXer-Based Time Series Forecasting: Detailed Improvement Plan

---

## üìå Project Goal

Enhance the TimeXer architecture to improve performance and robustness in:
- Diverse temporal patterns (short- and long-term)
- High-dimensional datasets (e.g., Traffic)
- Spike prediction (sudden value changes)
- Dynamic exogenous variable fusion
- Cross-variable interaction learning

We will build upon the original [TimeXer GitHub repo](https://github.com/thuml/TimeXer) and progressively enhance its architecture and training process.

---

## üéØ Motivation

Although TimeXer achieves competitive results using fixed-length patches, its use of a **uniform patch length = 16 for all long-term forecasting datasets** (e.g., ETT, Traffic, Weather, Electricity) limits flexibility. In our pre-experiments, we observed that:
- Patch length = 8 or 24 sometimes outperformed 16 on specific datasets.
- No single patch size works best across all datasets or prediction lengths.
- TimeXer does not attempt to **adapt** patch resolution.

Additionally:
- No channel selection: leads to poor performance on datasets like Traffic (862 channels).
- Poor spike prediction: MAE may look fine, but MSE is large due to occasional spikes.
- Exogenous variables are fused only via a global token, ignoring finer dynamics.
- Variable-to-variable relations are not modeled (unlike iTransformer).

These motivate our modular improvements below.

---

## üõ†Ô∏è Proposed Modules

### ‚úÖ M1. Multi-Scale Patch Tokenization
- Use multiple patch sizes (e.g., 8, 16, 24)
- Fuse multi-resolution temporal tokens

### ‚úÖ M2. Learnable Patch Fusion
- Attention/gating-based fusion of tokens from different patch scales

### ‚úÖ M3. Channel Masking / Gating
- SE-block-style module to suppress uninformative channels

### ‚úÖ M4. Spike-aware Loss Function
- Add loss weight to high-error or high-derivative regions

### ‚úÖ M5. Enhanced Exogenous Fusion
- Apply exo-to-patch cross attention at multiple encoder layers

### ‚úÖ M6. Channel-Wise Attention & Fusion Head
- Add iTransformer-style variate fusion to final encoder output

---

## üìä Baseline: TimeXer Results on Datasets (Patch Length = 16)

| Dataset     | Horizon | MAE   | MSE   |
|-------------|---------|-------|-------|
| ETTm2       | 96      | 0.188 | 0.051 |
| ETTh1       | 96      | 0.384 | 0.300 |
| Weather     | 96      | 0.158 | 0.035 |
| Traffic     | 96      | 0.418 | 0.281 |
| Electricity | 96      | 0.127 | 0.026 |

> Source: TimeXer Appendix G. For long-term forecasting tasks, PL=16 is used with input length = 96.

---

## üî¨ Experimental Setup

### Datasets
- ETTh1, ETTm2 (long-term)
- Traffic (high-dimension)
- Weather, Electricity (multivariate)

### Baselines
- TimeXer (PL=16)
- iTransformer
- PatchTST

### Ablation Studies
| Variant     | Modules Active             |
|-------------|----------------------------|
| TimeXer     | Baseline only              |
| +M1         | Multi-scale patch only     |
| +M1 + M3    | Patch + Channel Mask       |
| +M1 + M4    | Patch + Spike-aware Loss   |
| +All        | Full model (M1~M6)         |

---

## üß™ Development Phases

### Phase 1: Reproduce Baseline
- Run TimeXer with PL=16
- Validate logging and metrics

### Phase 2: Integrate Modules
1. Add M1 (multi-patch tokenizer)
2. Add M3 (channel masking)
3. Add M4 (spike-aware loss)
4. Add M5 (exo fusion)
5. Add M6 (channel-wise fusion)

### Phase 3: Benchmark + Ablation
- Compare accuracy, spike error
- Visualize learned masks/weights

### Phase 4: Final Report
- Table/chart output
- Model diagram
- Codebase release

---

## üì¶ Deliverables

- Enhanced TimeXer repository
- Experiment logs + tables
- Final report
- Presentation slides

---

## ‚≠ê Summary

This project upgrades TimeXer by introducing adaptivity in temporal resolution, channel selection, exogenous fusion, spike sensitivity, and variate reasoning. It combines the best of TimeXer, Pathformer, and iTransformer.
