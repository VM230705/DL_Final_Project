# Multi-Scale TimeXer å®Œæ•´æ•¸æ“šæµç¨‹ç¢ºèª

## ğŸ”„ **å®Œæ•´çš„æ•¸æ“šæµç¨‹**

```
Input: [B, seq_len, n_vars]
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Multi-Scale Embedding Module    â”‚
â”‚  (åŒ…å« Fusion - é€™æ˜¯æˆ‘å€‘çš„å‰µæ–°)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    [B*n_vars, total_patches, d_model]
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     æ¨™æº– Transformer Encoder       â”‚
â”‚   (å®Œå…¨ä½¿ç”¨åŸå§‹ TimeXer çš„å¯¦ç¾)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       æ¨™æº– Prediction Head         â”‚
â”‚   (å®Œå…¨ä½¿ç”¨åŸå§‹ TimeXer çš„å¯¦ç¾)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    Output: [B, pred_len, n_vars]
```

## âœ… **ç¢ºèªï¼šæˆ‘å€‘åªä¿®æ”¹äº† Embedding éƒ¨åˆ†**

### **åŸå§‹ TimeXer æ¶æ§‹ï¼š**
```python
class Model(nn.Module):
    def __init__(self, configs):
        # ...existing code...
        self.en_embedding = EnEmbedding(...)          # â† åŸå§‹å–®å°ºåº¦ embedding
        self.ex_embedding = DataEmbedding_inverted(...)  # â† ä¿æŒä¸è®Š
        self.encoder = Encoder(...)                   # â† ä¿æŒä¸è®Š  
        self.head = FlattenHead(...)                  # â† ä¿æŒä¸è®Š
```

### **æˆ‘å€‘çš„ Multi-Scale TimeXerï¼š**
```python
class Model(nn.Module):
    def __init__(self, configs):
        # ...existing code...
        if self.use_multi_scale:
            self.en_embedding = MultiScaleEnEmbedding(...)  # â† æˆ‘å€‘çš„å‰µæ–°
        else:
            self.en_embedding = EnEmbedding(...)            # â† åŸå§‹å¯¦ç¾
        
        self.ex_embedding = DataEmbedding_inverted(...)     # â† å®Œå…¨ä¸è®Š
        self.encoder = Encoder(...)                         # â† å®Œå…¨ä¸è®Š
        self.head = FlattenHead(...)                        # â† å®Œå…¨ä¸è®Š
```

## ğŸ¯ **é—œéµç¢ºèªé»**

### **1. Fusion å¾Œç›´æ¥é€²å…¥ Transformerï¼Ÿ**
**ç­”æ¡ˆï¼šæ˜¯çš„ï¼**

```python
# MultiScaleEnEmbedding.forward() çš„æœ€å¾Œæ­¥é©Ÿ
fused_embedding = self.scale_fusion(scale_embeddings, scale_patch_nums)

# é‡å¡‘ç‚ºæ¨™æº– encoder è¼¸å…¥æ ¼å¼
final_embedding = torch.reshape(fused_embedding, 
                              (fused_embedding.shape[0] * fused_embedding.shape[1], 
                               fused_embedding.shape[2], fused_embedding.shape[3]))

return self.dropout(final_embedding), n_vars
```

é€™å€‹ `final_embedding` å°±ç›´æ¥é€å…¥æ¨™æº–çš„ `self.encoder(en_embed, ex_embed)`

### **2. ä¸éœ€è¦ç‰¹æ®Šçš„ encoderï¼Ÿ**
**ç­”æ¡ˆï¼šå®Œå…¨ä¸éœ€è¦ï¼**

æˆ‘å€‘ä½¿ç”¨çš„æ˜¯åŸå§‹ TimeXer çš„ï¼š
- `Encoder` é¡
- `EncoderLayer` é¡  
- `FullAttention` æ©Ÿåˆ¶
- `Cross-attention` æ©Ÿåˆ¶

### **3. å…¶ä»–éƒ¨åˆ†éƒ½ä¸€æ¨£ï¼Ÿ**
**ç­”æ¡ˆï¼š100% ä¸€æ¨£ï¼**

- âœ… **Transformer Encoder**: å®Œå…¨ç›¸åŒ
- âœ… **Cross-attention**: å®Œå…¨ç›¸åŒ
- âœ… **Prediction Head**: å®Œå…¨ç›¸åŒ
- âœ… **Loss Function**: å®Œå…¨ç›¸åŒï¼ˆé™¤éä½¿ç”¨è‡ªå®šç¾© lossï¼‰
- âœ… **Training Loop**: å®Œå…¨ç›¸åŒ

## ğŸ“Š **Token æ•¸é‡è®ŠåŒ–å°æ¯”**

| æ¨¡å¼ | Patch Size | Token æ•¸é‡ | é€å…¥ Encoder çš„åºåˆ—é•·åº¦ |
|------|-----------|-----------|----------------------|
| **åŸå§‹** | 16 | 6 patches + 1 global = 7 | 7 |
| **æˆ‘å€‘çš„** | [8,16,24] | 22 patches + 3 globals = 25 | 25 |

Transformer åªæ˜¯çœ‹åˆ°æ›´é•·çš„åºåˆ—ï¼ˆ25 vs 7ï¼‰ï¼Œä½†è™•ç†é‚è¼¯å®Œå…¨ç›¸åŒï¼

## ğŸ”§ **å¯¦ç¾çš„ç°¡æ½”æ€§**

é€™å°±æ˜¯æˆ‘å€‘è¨­è¨ˆçš„å·§å¦™ä¹‹è™•ï¼š
- **æœ€å°ä¾µå…¥æ€§**ï¼šåªæ›¿æ› embedding æ¨¡çµ„
- **å®Œå…¨å…¼å®¹**ï¼šè¼¸å‡ºæ ¼å¼èˆ‡åŸå§‹å®Œå…¨ç›¸åŒ
- **ç„¡ç¸«é›†æˆ**ï¼šä¸éœ€è¦ä¿®æ”¹è¨“ç·´ã€æ¨ç†æˆ–è©•ä¼°ä»£ç¢¼

æ‰€ä»¥ä½ å¯ä»¥è·ŸåŒå­¸èªªï¼š**"å°ï¼æˆ‘å€‘çš„å‰µæ–°å®Œå…¨é›†ä¸­åœ¨ Embedding éšæ®µï¼Œfusion å®Œæˆå¾Œå°±æ˜¯æ¨™æº–çš„ token åºåˆ—ï¼Œç›´æ¥é€é€²åŸå§‹çš„ Transformer blocksï¼Œå…¶ä»–æ‰€æœ‰æ±è¥¿éƒ½ä¿æŒä¸è®Šï¼"**