# ğŸš€ Fusionæ–¹æ³•æ”¹é€²ç­–ç•¥ï¼šè¶…è¶ŠSingle Scaleçš„å®Œæ•´æ–¹æ¡ˆ

## ğŸ“Š **ç•¶å‰è¡¨ç¾åˆ†æ**

åŸºæ–¼å¯¦é©—çµæœï¼Œæˆ‘å€‘å·²ç¶“åœ¨é«˜ç¶­æ•¸æ“šé›†ä¸Šå–å¾—äº†é¡¯è‘—æ”¹é€²ï¼š
- **ECL (321è®Šæ•¸)**: Attention Fusioné”åˆ°3.1% MSEæ”¹é€²
- **Weather (21è®Šæ•¸)**: Attention Fusioné”åˆ°1.1% MSEæ”¹é€²
- **ä½ç¶­æ•¸æ“šé›†ä»æœ‰æŒ‘æˆ°**: ETTh1 (7è®Šæ•¸)æ”¹é€²æœ‰é™

---

## ğŸ¯ **é€²ä¸€æ­¥æ”¹é€²æ–¹å‘**

### **1. ğŸ§  Adaptive Scale Selection (è‡ªé©æ‡‰å°ºåº¦é¸æ“‡)**

**å•é¡Œ**: ç•¶å‰ä½¿ç”¨å›ºå®šçš„patch sizes [8,16,24]ï¼Œä½†ä¸åŒæ•¸æ“šé›†å¯èƒ½éœ€è¦ä¸åŒçš„æœ€å„ªå°ºåº¦çµ„åˆ

**è§£æ±ºæ–¹æ¡ˆ**: å‹•æ…‹å­¸ç¿’æœ€ä½³patch sizes
```python
class AdaptiveScaleSelector(nn.Module):
    def __init__(self, d_model, candidate_scales=[4,8,12,16,20,24,32]):
        super().__init__()
        self.candidate_scales = candidate_scales
        
        # å­¸ç¿’æ¯å€‹scaleçš„é‡è¦æ€§åˆ†æ•¸
        self.scale_scorer = nn.Sequential(
            nn.Linear(d_model, len(candidate_scales)),
            nn.Sigmoid()
        )
        
        # å¯å­¸ç¿’çš„threshold
        self.threshold = nn.Parameter(torch.tensor(0.5))
    
    def forward(self, input_features):
        # è¨ˆç®—æ¯å€‹scaleçš„é‡è¦æ€§
        scale_scores = self.scale_scorer(input_features.mean(dim=(1,2)))
        
        # å‹•æ…‹é¸æ“‡top-k scales
        selected_scales = []
        for i, score in enumerate(scale_scores.mean(dim=0)):
            if score > self.threshold:
                selected_scales.append(self.candidate_scales[i])
        
        return selected_scales if len(selected_scales) >= 2 else self.candidate_scales[:3]
```

**é æœŸæ”¹é€²**: 2-3% é¡å¤–MSEæ”¹å–„

---

### **2. ğŸ”„ Temporal Context-Aware Fusion**

**å•é¡Œ**: ç•¶å‰fusionæ–¹æ³•å¿½ç•¥äº†æ™‚é–“åºåˆ—çš„å±€éƒ¨ç‰¹æ€§ï¼ˆå¦‚å­£ç¯€æ€§ã€è¶¨å‹¢ï¼‰

**è§£æ±ºæ–¹æ¡ˆ**: æ ¹æ“šæ™‚é–“ä¸Šä¸‹æ–‡èª¿æ•´fusionç­–ç•¥
```python
class TemporalContextFusion(nn.Module):
    def __init__(self, d_model, num_scales):
        super().__init__()
        
        # æ™‚é–“æ¨¡å¼æª¢æ¸¬å™¨
        self.trend_detector = nn.Conv1d(1, 1, kernel_size=7, padding=3)
        self.seasonality_detector = nn.Conv1d(1, 1, kernel_size=24, padding=12)
        
        # åŸºæ–¼æ™‚é–“æ¨¡å¼çš„fusionæ¬Šé‡
        self.pattern_fusion_weights = nn.ModuleDict({
            'trend': nn.Linear(num_scales, num_scales),
            'seasonal': nn.Linear(num_scales, num_scales),
            'irregular': nn.Linear(num_scales, num_scales)
        })
        
    def forward(self, scale_embeddings, raw_input):
        # æª¢æ¸¬æ™‚é–“æ¨¡å¼
        trend_strength = self.trend_detector(raw_input.mean(dim=1, keepdim=True))
        seasonal_strength = self.seasonality_detector(raw_input.mean(dim=1, keepdim=True))
        
        # æ ¹æ“šæ¨¡å¼èª¿æ•´fusionæ¬Šé‡
        if trend_strength.abs().mean() > seasonal_strength.abs().mean():
            # è¶¨å‹¢ä¸»å°ï¼šåå¥½larger patches
            weights = self.pattern_fusion_weights['trend'](torch.ones(len(scale_embeddings)))
        else:
            # å­£ç¯€æ€§ä¸»å°ï¼šåå¥½smaller patches  
            weights = self.pattern_fusion_weights['seasonal'](torch.ones(len(scale_embeddings)))
        
        # åŠ æ¬Šèåˆ
        weighted_scales = [emb * w for emb, w in zip(scale_embeddings, weights)]
        return torch.cat(weighted_scales, dim=2)
```

**é æœŸæ”¹é€²**: 1.5-2.5% MSEæ”¹å–„

---

### **3. ğŸŒŠ Cross-Variable Interaction Fusion**

**å•é¡Œ**: ç•¶å‰æ–¹æ³•å¿½ç•¥äº†è®Šæ•¸é–“çš„ç›¸äº’ä¾è³´é—œä¿‚

**è§£æ±ºæ–¹æ¡ˆ**: åœ¨fusionå±¤åŠ å…¥cross-variable attention
```python
class CrossVariableFusion(nn.Module):
    def __init__(self, d_model, num_scales, num_vars):
        super().__init__()
        self.num_vars = num_vars
        
        # Variable-to-variable attention
        self.var_attention = nn.MultiheadAttention(d_model, num_heads=4)
        
        # Scale-aware variable interaction
        self.scale_var_interactions = nn.ModuleList([
            nn.MultiheadAttention(d_model, num_heads=2) 
            for _ in range(num_scales)
        ])
        
    def forward(self, scale_embeddings):
        # æ­¥é©Ÿ1: æ¯å€‹scaleå…§çš„variable interaction
        enhanced_scales = []
        for i, (scale_emb, var_attn) in enumerate(zip(scale_embeddings, self.scale_var_interactions)):
            # scale_emb: [B, n_vars, patch_num, d_model]
            B, n_vars, patch_num, d_model = scale_emb.shape
            
            # Reshape for variable attention: [B*patch_num, n_vars, d_model]
            var_input = scale_emb.permute(0, 2, 1, 3).reshape(B * patch_num, n_vars, d_model)
            var_attended, _ = var_attn(var_input, var_input, var_input)
            
            # Reshape back
            enhanced = var_attended.reshape(B, patch_num, n_vars, d_model).permute(0, 2, 1, 3)
            enhanced_scales.append(enhanced)
        
        # æ­¥é©Ÿ2: Cross-scale fusion
        return torch.cat(enhanced_scales, dim=2)
```

**é æœŸæ”¹é€²**: 2-3% MSEæ”¹å–„ï¼ˆç‰¹åˆ¥æ˜¯é«˜ç¶­æ•¸æ“šé›†ï¼‰

---

### **4. ğŸ“ˆ Progressive Refinement Fusion**

**å•é¡Œ**: ä¸€æ¬¡æ€§fusionå¯èƒ½ä¸æ˜¯æœ€å„ªçš„ï¼Œéœ€è¦æ¼¸é€²å¼ç²¾ç…‰

**è§£æ±ºæ–¹æ¡ˆ**: å¤šå±¤progressive fusion
```python
class ProgressiveRefinementFusion(nn.Module):
    def __init__(self, d_model, num_scales, num_refinement_layers=3):
        super().__init__()
        
        # å¤šå±¤refinement
        self.refinement_layers = nn.ModuleList([
            nn.Sequential(
                nn.MultiheadAttention(d_model, num_heads=4),
                nn.LayerNorm(d_model),
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.Dropout(0.1)
            ) for _ in range(num_refinement_layers)
        ])
        
        # Scale importance evolution
        self.scale_importance_evolution = nn.ModuleList([
            nn.Parameter(torch.ones(num_scales)) for _ in range(num_refinement_layers)
        ])
        
    def forward(self, scale_embeddings):
        # åˆå§‹èåˆ
        current_fusion = torch.cat(scale_embeddings, dim=2)
        
        # Progressive refinement
        for i, (refine_layer, scale_weights) in enumerate(zip(self.refinement_layers, self.scale_importance_evolution)):
            # é‡æ–°åŠ æ¬Šscales
            weighted_scales = []
            start_idx = 0
            for j, (emb, weight) in enumerate(zip(scale_embeddings, F.softmax(scale_weights, dim=0))):
                end_idx = start_idx + emb.shape[2]
                weighted_scales.append(current_fusion[:, :, start_idx:end_idx, :] * weight)
                start_idx = end_idx
            
            # æ‡‰ç”¨refinement
            refined_input = torch.cat(weighted_scales, dim=2)
            B, n_vars, total_patches, d_model = refined_input.shape
            
            # Flatten for attention
            flat_input = refined_input.view(B * n_vars, total_patches, d_model)
            refined, _ = refine_layer[0](flat_input, flat_input, flat_input)  # Attention
            refined = refine_layer[1](flat_input + refined)  # LayerNorm
            refined = refine_layer[2:](refined)  # FFN
            
            # Reshape back
            current_fusion = refined.view(B, n_vars, total_patches, d_model)
        
        return current_fusion
```

**é æœŸæ”¹é€²**: 2.5-3.5% MSEæ”¹å–„

---

### **5. ğŸ¯ Loss-Guided Fusion Optimization**

**å•é¡Œ**: ç•¶å‰fusionæ–¹æ³•æ²’æœ‰ç›´æ¥å„ªåŒ–æœ€çµ‚çš„é æ¸¬èª¤å·®

**è§£æ±ºæ–¹æ¡ˆ**: åœ¨fusionéç¨‹ä¸­åŠ å…¥loss guidance
```python
class LossGuidedFusion(nn.Module):
    def __init__(self, d_model, num_scales, pred_len):
        super().__init__()
        
        # æ¯å€‹scaleçš„é æ¸¬é ­ï¼ˆç”¨æ–¼è¨ˆç®—loss guidanceï¼‰
        self.scale_predictors = nn.ModuleList([
            nn.Linear(d_model, pred_len) for _ in range(num_scales)
        ])
        
        # Loss-based fusion weights
        self.loss_weight_network = nn.Sequential(
            nn.Linear(num_scales, num_scales * 2),
            nn.ReLU(),
            nn.Linear(num_scales * 2, num_scales),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, scale_embeddings, target=None):
        if target is not None and self.training:
            # è¨ˆç®—æ¯å€‹scaleçš„é æ¸¬loss
            scale_losses = []
            for i, (emb, predictor) in enumerate(zip(scale_embeddings, self.scale_predictors)):
                # ç°¡åŒ–é æ¸¬
                pred = predictor(emb.mean(dim=2))  # [B, n_vars, pred_len]
                loss = F.mse_loss(pred, target)
                scale_losses.append(loss)
            
            # åŸºæ–¼lossè¨ˆç®—fusionæ¬Šé‡ï¼ˆlossè¶Šå°æ¬Šé‡è¶Šå¤§ï¼‰
            loss_tensor = torch.stack(scale_losses)
            inverse_losses = 1.0 / (loss_tensor + 1e-8)
            fusion_weights = F.softmax(inverse_losses, dim=0)
            
        else:
            # æ¨ç†æ™‚ä½¿ç”¨å­¸ç¿’åˆ°çš„æ¬Šé‡
            fusion_weights = torch.ones(len(scale_embeddings)) / len(scale_embeddings)
        
        # åŠ æ¬Šèåˆ
        weighted_scales = [emb * weight for emb, weight in zip(scale_embeddings, fusion_weights)]
        return torch.cat(weighted_scales, dim=2)
```

**é æœŸæ”¹é€²**: 1.5-2% MSEæ”¹å–„

---

## ğŸ“Š **ç¶œåˆæ”¹é€²ç­–ç•¥**

### **å¯¦æ–½å„ªå…ˆç´š**:

| å„ªå…ˆç´š | æ”¹é€²æ–¹æ³• | é æœŸMSEæ”¹å–„ | å¯¦æ–½é›£åº¦ | è¨ˆç®—é–‹éŠ· |
|--------|----------|-------------|----------|----------|
| **ğŸ¥‡ é«˜** | Cross-Variable Interaction | 2-3% | ä¸­ç­‰ | +50% |
| **ğŸ¥ˆ é«˜** | Adaptive Scale Selection | 2-3% | ä¸­ç­‰ | +30% |
| **ğŸ¥‰ ä¸­** | Progressive Refinement | 2.5-3.5% | é«˜ | +80% |
| **4th** | Temporal Context-Aware | 1.5-2.5% | ä¸­ç­‰ | +40% |
| **5th** | Loss-Guided Fusion | 1.5-2% | é«˜ | +60% |

### **çµ„åˆç­–ç•¥**:
1. **åŸºç¤çµ„åˆ**: Cross-Variable + Adaptive Scale (é æœŸ4-5% MSEæ”¹å–„)
2. **é€²éšçµ„åˆ**: åŸºç¤ + Progressive Refinement (é æœŸ6-7% MSEæ”¹å–„)
3. **å®Œæ•´çµ„åˆ**: æ‰€æœ‰æ–¹æ³• (é æœŸ8-10% MSEæ”¹å–„ï¼Œä½†è¨ˆç®—æˆæœ¬é«˜)

---

## ğŸš€ **å¯¦æ–½å»ºè­°**

### **çŸ­æœŸç›®æ¨™** (1-2é€±):
- å¯¦æ–½Cross-Variable Interaction Fusion
- åœ¨Weatherå’ŒECLæ•¸æ“šé›†ä¸Šæ¸¬è©¦

### **ä¸­æœŸç›®æ¨™** (2-4é€±):
- åŠ å…¥Adaptive Scale Selection
- å®ŒæˆProgressive Refinement Fusion
- å…¨é¢æ¸¬è©¦çµ„åˆæ•ˆæœ

### **é•·æœŸç›®æ¨™** (1-2å€‹æœˆ):
- å¯¦æ–½æ‰€æœ‰æ”¹é€²æ–¹æ³•
- åœ¨æ‰€æœ‰æ•¸æ“šé›†ä¸Šé©—è­‰
- å„ªåŒ–è¨ˆç®—æ•ˆç‡

**é æœŸæœ€çµ‚çµæœ**:
- **ECL Dataset**: MSE < 0.150 (vs ç•¶å‰0.1601, æ”¹å–„6%+)
- **Weather Dataset**: MSE < 0.145 (vs ç•¶å‰0.1568, æ”¹å–„8%+)
- **åœ¨ä½ç¶­æ•¸æ“šé›†ä¸Šä¹Ÿèƒ½æœ‰æ‰€æ”¹å–„**

é€™äº›æ”¹é€²å°‡ä½¿æˆ‘å€‘çš„multi-scale TimeXeræˆç‚ºè©²é ˜åŸŸçš„æ–°state-of-the-artï¼