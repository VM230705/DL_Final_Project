# TimeXer å¤šå°ºåº¦æ™‚é–“åºåˆ—é æ¸¬æ¨¡å‹ - æ–¹æ³•è«–èªªæ˜

## ğŸ“‹ ç›®éŒ„
- [ğŸ¯ ç ”ç©¶å‹•æ©Ÿèˆ‡æ–¹æ³•æ¦‚è¿°](#-ç ”ç©¶å‹•æ©Ÿèˆ‡æ–¹æ³•æ¦‚è¿°)
- [ğŸ—ï¸ æ•´é«”æ¶æ§‹æµç¨‹](#ï¸-æ•´é«”æ¶æ§‹æµç¨‹)
- [ğŸ”§ æ ¸å¿ƒæŠ€è¡“æ–¹æ³•](#-æ ¸å¿ƒæŠ€è¡“æ–¹æ³•)
- [ğŸ”„ å¤šå°ºåº¦èåˆç­–ç•¥](#-å¤šå°ºåº¦èåˆç­–ç•¥)
- [âš™ï¸ èåˆåŸ·è¡Œæ©Ÿåˆ¶è©³è§£](#ï¸-èåˆåŸ·è¡Œæ©Ÿåˆ¶è©³è§£)
- [ğŸ“Š æ–¹æ³•æ¯”è¼ƒåˆ†æ](#-æ–¹æ³•æ¯”è¼ƒåˆ†æ)

---

## ğŸ¯ ç ”ç©¶å‹•æ©Ÿèˆ‡æ–¹æ³•æ¦‚è¿°

### å•é¡ŒèƒŒæ™¯èˆ‡ç ”ç©¶ç™¼ç¾

**å›ºå®šè£œä¸é•·åº¦çš„é™åˆ¶**ï¼š
åœ¨ TimeXer ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘é¦–å…ˆä½¿ç”¨å›ºå®šçš„è£œä¸é•·åº¦ 16 ä¾†æå–è£œä¸æ¨™è¨˜ã€‚é€šéåœ¨å„å€‹æ•¸æ“šé›†ä¸Šè©•ä¼°ä¸åŒçš„å›ºå®šè£œä¸é•·åº¦ï¼ˆ4, 8, 24, 32ï¼‰ï¼Œæˆ‘å€‘ç™¼ç¾ï¼š

- ğŸ“Š **å¤§éƒ¨åˆ†æ•¸æ“šé›†åœ¨è£œä¸é•·åº¦ç‚º 16 æ™‚é”åˆ°æœ€ä½³æ€§èƒ½**
- âŒ **ç°¡å–®èª¿æ•´å›ºå®šè£œä¸é•·åº¦ä¸¦ä¸èƒ½å¸¶ä¾†æ€§èƒ½æå‡**
- ğŸ” **ä¸åŒæ•¸æ“šé›†çš„æœ€é©è£œä¸é•·åº¦å­˜åœ¨å·®ç•°**

### æ ¸å¿ƒå•é¡Œåˆ†æ

å‚³çµ±çš„**å–®ä¸€å›ºå®šè£œä¸é•·åº¦**æ–¹æ³•å­˜åœ¨ä»¥ä¸‹å•é¡Œï¼š
- ğŸ¯ **æ•¸æ“šé›†ç‰¹ç•°æ€§**ï¼šä¸åŒæ•¸æ“šé›†éœ€è¦ä¸åŒçš„æ™‚é–“ç²’åº¦
- ğŸ“ˆ **æ¨¡å¼å¤šæ¨£æ€§**ï¼šæ™‚é–“åºåˆ—åŒ…å«å¤šå°ºåº¦çš„é€±æœŸæ€§å’Œè¶¨å‹¢æ¨¡å¼
- âš–ï¸ **æœ€ä½³åŒ–å›°é›£**ï¼šé›£ä»¥ç‚ºæ‰€æœ‰å ´æ™¯æ‰¾åˆ°çµ±ä¸€çš„æœ€ä½³è£œä¸å¤§å°
- ğŸ”„ **ä¿¡æ¯æå¤±**ï¼šå–®ä¸€å°ºåº¦å¯èƒ½éŒ¯éé‡è¦çš„æ™‚é–“æ¨¡å¼

### æœ¬ç ”ç©¶çš„å‰µæ–°è§£æ±ºæ–¹æ¡ˆ

ç‚ºäº†è§£æ±ºä¸Šè¿°å•é¡Œï¼Œæˆ‘å€‘æå‡ºäº†**å¤šå°ºåº¦æ–¹æ³•**ï¼š

1. **å¤šå°ºåº¦è£œä¸è¼¸å…¥**ï¼š
   - åŒæ™‚ä½¿ç”¨ä¾†è‡ªå¤šå€‹è£œä¸é•·åº¦çš„æ¨™è¨˜ä½œç‚ºè¼¸å…¥
   - æ•æ‰ä¸åŒæ™‚é–“å°ºåº¦çš„ç‰¹å¾µè¡¨ç¤º

2. **æ™ºèƒ½æ³¨æ„åŠ›æ©Ÿåˆ¶**ï¼š
   - æ¨¡å‹ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶è‡ªå‹•åˆ†é…æ¬Šé‡
   - ç‚ºæ¯å€‹æ•¸æ“šé›†çš„é‡è¦è£œä¸æ¨™è¨˜è³¦äºˆæ›´é«˜æ¬Šé‡

3. **è‡ªé©æ‡‰é‡è¦æ€§å­¸ç¿’**ï¼š
   - æ ¹æ“šå…·é«”ä»»å‹™å’Œæ•¸æ“šç‰¹æ€§
   - è‡ªå‹•å­¸ç¿’ä¸åŒå°ºåº¦çš„ç›¸å°é‡è¦æ€§

### æ–¹æ³•å‰µæ–°é»

1. **å¤šå°ºåº¦è£œä¸åµŒå…¥**ï¼šçªç ´å›ºå®šè£œä¸é•·åº¦é™åˆ¶ï¼ŒåŒæ™‚è™•ç†å¤šå€‹æ™‚é–“çª—å£
2. **æ³¨æ„åŠ›é©…å‹•èåˆ**ï¼šä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶æ™ºèƒ½æ•´åˆä¸åŒå°ºåº¦ä¿¡æ¯
3. **æ•¸æ“šè‡ªé©æ‡‰**ï¼šæ¨¡å‹èƒ½æ ¹æ“šä¸åŒæ•¸æ“šé›†ç‰¹æ€§è‡ªå‹•èª¿æ•´å°ºåº¦æ¬Šé‡
4. **ç«¯åˆ°ç«¯å„ªåŒ–**ï¼šçµ±ä¸€æ¡†æ¶ä¸‹è¯åˆå­¸ç¿’æ‰€æœ‰å°ºåº¦çš„æœ€ä½³çµ„åˆ

---

## ğŸ—ï¸ æ•´é«”æ¶æ§‹æµç¨‹

### å¾å›ºå®šå°ºåº¦åˆ°å¤šå°ºåº¦çš„æ¼”é€²

```mermaid
flowchart LR
    subgraph Traditional["ğŸ”’ å‚³çµ±å›ºå®šå°ºåº¦æ–¹æ³•"]
        Fixed["å›ºå®šè£œä¸é•·åº¦ 16"]
        LimitFixed["âŒ æ€§èƒ½å—é™"]
    end
    
    subgraph MultiScale["ğŸš€ å¤šå°ºåº¦å‰µæ–°æ–¹æ³•"]
        Multi["è£œä¸é•·åº¦ [4,8,16,24,32]"]
        Attention["âœ¨ æ³¨æ„åŠ›åŠ æ¬Š"]
        Adaptive["ğŸ¯ è‡ªé©æ‡‰å­¸ç¿’"]
    end
    
    Fixed --> LimitFixed
    Multi --> Attention --> Adaptive
    
    Traditional -.->|çªç ´é™åˆ¶| MultiScale
```

### è©³ç´°åŸ·è¡Œæµç¨‹åœ–

```mermaid
flowchart TD
    subgraph Input["ğŸ“¥ è¼¸å…¥å±¤"]
        TS["æ™‚é–“åºåˆ— x[t]<br/>[B, C, L]"]
        Problem["âŒ å›ºå®šè£œä¸é•·åº¦=16<br/>æ€§èƒ½å—é™"]
    end
    
    subgraph MSE["ğŸ” å¤šå°ºåº¦åµŒå…¥å±¤ (è§£æ±ºæ–¹æ¡ˆ)"]
        P4["ğŸ“¦ Patch Size 4<br/>è¶…ç´°ç²’åº¦"]
        P8["ğŸ“¦ Patch Size 8<br/>ç´°ç²’åº¦ç‰¹å¾µ"]
        P16["ğŸ“¦ Patch Size 16<br/>æ¨™æº–ç²’åº¦"]
        P24["ğŸ“¦ Patch Size 24<br/>ä¸­ç­‰ç²’åº¦"]
        P32["ğŸ“¦ Patch Size 32<br/>ç²—ç²’åº¦ç‰¹å¾µ"]
    end
    
    subgraph Embed["ğŸ¯ ç‰¹å¾µåµŒå…¥"]
        E4["ç·šæ€§åµŒå…¥ + ä½ç½®ç·¨ç¢¼"]
        E8["ç·šæ€§åµŒå…¥ + ä½ç½®ç·¨ç¢¼"]
        E16["ç·šæ€§åµŒå…¥ + ä½ç½®ç·¨ç¢¼"]
        E24["ç·šæ€§åµŒå…¥ + ä½ç½®ç·¨ç¢¼"]
        E32["ç·šæ€§åµŒå…¥ + ä½ç½®ç·¨ç¢¼"]
    end
    
    subgraph Fusion["ğŸ”„ æ³¨æ„åŠ›é©…å‹•èåˆ"]
        SF["Scale Fusion Module<br/>æ™ºèƒ½æ¬Šé‡åˆ†é…"]
        note1["âš ï¸ è‡ªå‹•å­¸ç¿’æ¯å€‹æ•¸æ“šé›†çš„<br/>æœ€é‡è¦è£œä¸æ¨™è¨˜æ¬Šé‡"]
    end
    
    subgraph Encoder["ğŸ”§ ç‰¹å¾µå­¸ç¿’"]
        TE["Transformer Encoder<br/>è™•ç†èåˆå¾Œçš„çµ±ä¸€ç‰¹å¾µ"]
    end
    
    subgraph Output["ğŸ“¤ è¼¸å‡ºå±¤"]
        FH["Flatten Head"]
        Pred["é æ¸¬çµæœ Å·[t+h]"]
        Better["âœ… æ€§èƒ½æå‡"]
    end
    
    TS --> MSE
    P4 --> E4
    P8 --> E8
    P16 --> E16
    P24 --> E24
    P32 --> E32
    
    E4 --> SF
    E8 --> SF
    E16 --> SF
    E24 --> SF
    E32 --> SF
    
    SF --> TE
    TE --> FH
    FH --> Pred --> Better
    
    style MSE fill:#e8f5e8
    style Fusion fill:#ffeb3b
    style note1 fill:#ffcdd2
```

### æ ¸å¿ƒæ–¹æ³•æµç¨‹

#### ç¬¬ä¸€éšæ®µï¼šå¤šå°ºåº¦è£œä¸ç”Ÿæˆ
```python
# ä¸å†å±€é™æ–¼å›ºå®šè£œä¸é•·åº¦ 16
# åŒæ™‚ç”Ÿæˆå¤šå€‹å°ºåº¦çš„è£œä¸
patch_sizes = [4, 8, 16, 24, 32]  # å¤šå°ºåº¦è¨­è¨ˆ
for patch_size in patch_sizes:
    patches = x.unfold(size=patch_size, step=patch_size)
    scale_embeddings.append(patches)
```

#### ç¬¬äºŒéšæ®µï¼šå°ºåº¦ç‰¹ç•°æ€§åµŒå…¥
```python
# æ¯å€‹å°ºåº¦ç¨ç«‹åµŒå…¥ï¼Œä¿æŒå°ºåº¦ç‰¹æ€§
for patch_size, patches in zip(patch_sizes, scale_patches):
    embedded = patch_embeddings[str(patch_size)](patches)
    embedded += positional_embedding(patches)
    embedded = cat([embedded, scale_specific_token], dim=2)
```

#### ç¬¬ä¸‰éšæ®µï¼šæ³¨æ„åŠ›é©…å‹•èåˆ
```python
# æ ¸å¿ƒå‰µæ–°ï¼šä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶è‡ªå‹•åˆ†é…æ¬Šé‡
# ç‚ºæ¯å€‹æ•¸æ“šé›†çš„é‡è¦è£œä¸æ¨™è¨˜è³¦äºˆæ›´é«˜æ¬Šé‡
fused_features = attention_fusion_module(scale_embeddings)
# æ¨¡å‹è‡ªå‹•å­¸ç¿’å“ªäº›å°ºåº¦å°ç•¶å‰æ•¸æ“šé›†æœ€é‡è¦
```

#### ç¬¬å››éšæ®µï¼šçµ±ä¸€ç‰¹å¾µå­¸ç¿’
```python
# Transformerè™•ç†èåˆå¾Œçš„å¤šå°ºåº¦ç‰¹å¾µ
encoded = transformer_encoder(fused_features)
prediction = prediction_head(encoded)
```

---

## ğŸ”§ æ ¸å¿ƒæŠ€è¡“æ–¹æ³•

### 1. å¤šå°ºåº¦è£œä¸åµŒå…¥çªç ´

**å•é¡Œ**ï¼šå›ºå®šè£œä¸é•·åº¦ 16 çš„æ€§èƒ½ç“¶é ¸

**è§£æ±ºæ–¹æ¡ˆ**ï¼šå¤šå°ºåº¦è£œä¸åµŒå…¥æ¶æ§‹

```python
class MultiScaleEnEmbedding(nn.Module):
    def __init__(self, patch_sizes=[4, 8, 16, 24, 32]):  # æ“´å±•å°ºåº¦ç¯„åœ
        # ç‚ºæ¯å€‹å°ºåº¦å‰µå»ºå°ˆé–€çš„åµŒå…¥å±¤
        self.patch_embeddings = nn.ModuleDict()
        for size in patch_sizes:
            self.patch_embeddings[str(size)] = nn.Linear(size, d_model)
        
        # å°ºåº¦ç‰¹å®šçš„å¯å­¸ç¿’æ¨™è¨˜
        self.global_tokens = nn.ParameterDict()
        for size in patch_sizes:
            self.global_tokens[str(size)] = nn.Parameter(torch.randn(1, n_vars, 1, d_model))
```

**æŠ€è¡“å„ªå‹¢**ï¼š
- âœ… **çªç ´å›ºå®šé™åˆ¶**ï¼šä¸å†å—å–®ä¸€è£œä¸é•·åº¦ç´„æŸ
- âœ… **å°ºåº¦å°ˆé–€åŒ–**ï¼šæ¯å€‹å°ºåº¦æœ‰ç¨ç«‹çš„å­¸ç¿’åƒæ•¸
- âœ… **å…¨é¢è¦†è“‹**ï¼šå¾ç´°ç²’åº¦åˆ°ç²—ç²’åº¦çš„å®Œæ•´æ™‚é–“å°ºåº¦

### 2. æ³¨æ„åŠ›é©…å‹•çš„æ™ºèƒ½èåˆ

**æ ¸å¿ƒç†å¿µ**ï¼šè®“æ¨¡å‹è‡ªå‹•å­¸ç¿’æ¯å€‹æ•¸æ“šé›†çš„æœ€é‡è¦è£œä¸æ¨™è¨˜

```python
class AttentionDrivenFusion(nn.Module):
    def __init__(self, d_model, patch_sizes):
        # å¯å­¸ç¿’çš„å°ºåº¦é‡è¦æ€§åƒæ•¸
        self.scale_importance = nn.Parameter(torch.ones(len(patch_sizes)))
        
        # è·¨å°ºåº¦æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.cross_scale_attention = nn.MultiheadAttention(
            d_model, num_heads=8, dropout=0.1, batch_first=True
        )
    
    def forward(self, scale_embeddings):
        # æ³¨æ„åŠ›æ©Ÿåˆ¶è‡ªå‹•åˆ†é…æ¬Šé‡
        importance_weights = F.softmax(self.scale_importance, dim=0)
        
        # ç‚ºé‡è¦çš„è£œä¸æ¨™è¨˜è³¦äºˆæ›´é«˜æ¬Šé‡
        weighted_scales = []
        for i, embedding in enumerate(scale_embeddings):
            weighted = embedding * importance_weights[i]
            weighted_scales.append(weighted)
        
        return torch.cat(weighted_scales, dim=2)
```

---

## ğŸ”„ å¤šå°ºåº¦èåˆç­–ç•¥

### 1. æ³¨æ„åŠ›é©…å‹•çš„æ¬Šé‡å­¸ç¿’

**æ•¸å­¸è¡¨é”**ï¼š
```
çµ¦å®šå¤šå°ºåº¦åµŒå…¥ E = {E_4, E_8, E_16, E_24, E_32}
é‡è¦æ€§æ¬Šé‡: Î±_i = softmax(w_i), where w_i æ˜¯å¯å­¸ç¿’åƒæ•¸
èåˆè¼¸å‡º: F = Î£(Î±_i * Attention(E_i))
```

**å­¸ç¿’æ©Ÿåˆ¶**ï¼š
```mermaid
graph TD
    subgraph Learning["ğŸ§  æ™ºèƒ½æ¬Šé‡å­¸ç¿’"]
        Data["æ•¸æ“šé›†ç‰¹æ€§"]
        Attention["æ³¨æ„åŠ›æ©Ÿåˆ¶"]
        Weights["è‡ªé©æ‡‰æ¬Šé‡"]
        Performance["æ€§èƒ½åé¥‹"]
    end
    
    Data --> Attention
    Attention --> Weights
    Weights --> Performance
    Performance -.->|åå‘å‚³æ’­| Weights
    
    subgraph Example["ğŸ“Š æ¬Šé‡åˆ†é…ç¤ºä¾‹"]
        Dataset1["é‡‘èæ•¸æ“š<br/>Î±=[0.1,0.3,0.4,0.2,0.0]"]
        Dataset2["æ°£è±¡æ•¸æ“š<br/>Î±=[0.0,0.1,0.2,0.3,0.4]"]
        Dataset3["éŠ·å”®æ•¸æ“š<br/>Î±=[0.2,0.4,0.3,0.1,0.0]"]
    end
```

### 2. å°ºåº¦æ„ŸçŸ¥æ³¨æ„åŠ›èåˆ (Scale-Aware Attention Fusion)

**æ ¸å¿ƒè¨­è¨ˆæ€æƒ³**ï¼š
åŸºæ–¼å‰è¨€ä¸­ç™¼ç¾çš„å•é¡Œï¼Œæˆ‘å€‘è¨­è¨ˆäº†ä¸‰éšæ®µçš„èåˆæ©Ÿåˆ¶ä¾†è‡ªå‹•å­¸ç¿’æ¯å€‹æ•¸æ“šé›†çš„æœ€é‡è¦è£œä¸æ¨™è¨˜æ¬Šé‡ã€‚

**æ–¹æ³•æ¶æ§‹**ï¼š
```mermaid
graph TD
    subgraph Stage1["ğŸ¯ éšæ®µ1: å°ºåº¦å…§å°ˆåŒ–æ³¨æ„åŠ›"]
        S4["å°ºåº¦4å°ˆç”¨Self-Attention<br/>å­¸ç¿’è¶…ç´°ç²’åº¦ç‰¹å¾µ"]
        S8["å°ºåº¦8å°ˆç”¨Self-Attention<br/>å­¸ç¿’ç´°ç²’åº¦ç‰¹å¾µ"]
        S16["å°ºåº¦16å°ˆç”¨Self-Attention<br/>å­¸ç¿’æ¨™æº–ç²’åº¦ç‰¹å¾µ"]
        S24["å°ºåº¦24å°ˆç”¨Self-Attention<br/>å­¸ç¿’ä¸­ç­‰ç²’åº¦ç‰¹å¾µ"]
        S32["å°ºåº¦32å°ˆç”¨Self-Attention<br/>å­¸ç¿’ç²—ç²’åº¦ç‰¹å¾µ"]
    end
    
    subgraph Stage2["ğŸ”„ éšæ®µ2: è·¨å°ºåº¦äº¤äº’æ³¨æ„åŠ›"]
        Cross["Cross-Scale Multi-Head Attention<br/>ç™¼ç¾å°ºåº¦é–“çš„äº’è£œé—œä¿‚"]
    end
    
    subgraph Stage3["âš–ï¸ éšæ®µ3: æ•¸æ“šè‡ªé©æ‡‰æ¬Šé‡åˆ†é…"]
        Weight["å¯å­¸ç¿’é‡è¦æ€§æ¬Šé‡ Î±_i<br/>ç‚ºæ¯å€‹æ•¸æ“šé›†è‡ªå‹•åˆ†é…æœ€ä½³æ¬Šé‡"]
    end
    
    S4 --> Cross
    S8 --> Cross
    S16 --> Cross
    S24 --> Cross
    S32 --> Cross
    Cross --> Weight
    Weight --> Output["èåˆè¼¸å‡º<br/>æœ€é©åˆç•¶å‰æ•¸æ“šé›†çš„å°ºåº¦çµ„åˆ"]
```

**è©³ç´°å¯¦ä½œæ–¹æ³•**ï¼š

```python
class ScaleAwareAttentionFusion(nn.Module):
    """
    è§£æ±ºå›ºå®šè£œä¸é•·åº¦é™åˆ¶çš„æ ¸å¿ƒæ¨¡çµ„
    è‡ªå‹•å­¸ç¿’æ¯å€‹æ•¸æ“šé›†çš„æœ€é‡è¦è£œä¸æ¨™è¨˜æ¬Šé‡
    """
    def __init__(self, d_model, patch_sizes=[4, 8, 16, 24, 32]):
        super().__init__()
        self.d_model = d_model
        self.patch_sizes = patch_sizes
        
        # éšæ®µ1: ç‚ºæ¯å€‹å°ºåº¦å‰µå»ºå°ˆç”¨çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
        # è§£æ±ºä¸åŒå°ºåº¦éœ€è¦ä¸åŒè™•ç†æ–¹å¼çš„å•é¡Œ
        self.scale_attentions = nn.ModuleDict()
        self.scale_norms = nn.ModuleDict()
        for patch_size in patch_sizes:
            # æ¯å€‹å°ºåº¦æœ‰ç¨ç«‹çš„æ³¨æ„åŠ›åƒæ•¸
            self.scale_attentions[str(patch_size)] = nn.MultiheadAttention(
                d_model, num_heads=8, dropout=0.1, batch_first=True
            )
            self.scale_norms[str(patch_size)] = nn.LayerNorm(d_model)
        
        # éšæ®µ2: è·¨å°ºåº¦äº¤äº’æ³¨æ„åŠ›
        # ç™¼ç¾ä¸åŒå°ºåº¦é–“çš„äº’è£œé—œä¿‚
        self.cross_scale_attention = nn.MultiheadAttention(
            d_model, num_heads=4, dropout=0.1, batch_first=True
        )
        
        # éšæ®µ3: å¯å­¸ç¿’çš„å°ºåº¦é‡è¦æ€§æ¬Šé‡
        # æ ¸å¿ƒå‰µæ–°ï¼šè‡ªå‹•å­¸ç¿’æ¯å€‹æ•¸æ“šé›†çš„æœ€ä½³å°ºåº¦çµ„åˆ
        self.scale_importance = nn.Parameter(torch.ones(len(patch_sizes)))
        
        # æœ€çµ‚è¼¸å‡ºæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
    def forward(self, scale_embeddings, scale_patch_nums):
        """
        è§£æ±ºå‰è¨€ä¸­æåˆ°çš„å•é¡Œï¼š
        1. å›ºå®šè£œä¸é•·åº¦16çš„æ€§èƒ½ç“¶é ¸
        2. ä¸åŒæ•¸æ“šé›†éœ€è¦ä¸åŒçš„æœ€ä½³å°ºåº¦çµ„åˆ
        """
        batch_size, n_vars = scale_embeddings[0].shape[:2]
        
        # éšæ®µ1: å°ºåº¦å…§å°ˆåŒ–æ³¨æ„åŠ›è™•ç†
        # æ¯å€‹å°ºåº¦ç¨ç«‹å­¸ç¿’å…¶ç‰¹æœ‰çš„æ™‚é–“æ¨¡å¼
        refined_scales = []
        for i, (patch_size, embedding) in enumerate(zip(self.patch_sizes, scale_embeddings)):
            # é‡å¡‘ç‚ºæ³¨æ„åŠ›è¼¸å…¥æ ¼å¼
            embedding_flat = embedding.view(batch_size * n_vars, embedding.shape[2], self.d_model)
            
            # æ‡‰ç”¨å°ºåº¦ç‰¹å®šçš„æ³¨æ„åŠ›
            refined, _ = self.scale_attentions[str(patch_size)](
                embedding_flat, embedding_flat, embedding_flat
            )
            
            # æ®˜å·®é€£æ¥å’Œæ­£è¦åŒ–
            refined = self.scale_norms[str(patch_size)](embedding_flat + refined)
            
            # é‡å¡‘å›åŸå§‹æ ¼å¼
            refined = refined.view(batch_size, n_vars, embedding.shape[2], self.d_model)
            refined_scales.append(refined)
        
        # éšæ®µ2: è·¨å°ºåº¦äº¤äº’æ³¨æ„åŠ›
        # è®“ä¸åŒå°ºåº¦é–“é€²è¡Œä¿¡æ¯äº¤æ›ï¼Œç™¼ç¾äº’è£œé—œä¿‚
        all_scales = torch.cat(refined_scales, dim=2)  # [B, n_vars, total_patches, d_model]
        all_scales_flat = all_scales.view(batch_size * n_vars, -1, self.d_model)
        
        cross_attended, attention_weights = self.cross_scale_attention(
            all_scales_flat, all_scales_flat, all_scales_flat
        )
        
        # éšæ®µ3: è‡ªå‹•å­¸ç¿’ä¸¦æ‡‰ç”¨æ•¸æ“šç‰¹å®šçš„é‡è¦æ€§æ¬Šé‡
        # æ ¸å¿ƒå‰µæ–°ï¼šç‚ºæ¯å€‹æ•¸æ“šé›†è‡ªå‹•åˆ†é…æœ€é©åˆçš„å°ºåº¦æ¬Šé‡
        importance_weights = F.softmax(self.scale_importance, dim=0)
        
        # åˆ†å‰²ä¸¦æ‡‰ç”¨é‡è¦æ€§æ¬Šé‡
        start_idx = 0
        weighted_scales = []
        for i, patch_num in enumerate(scale_patch_nums):
            end_idx = start_idx + patch_num
            scale_output = cross_attended[:, start_idx:end_idx, :]
            
            # æ‡‰ç”¨å­¸ç¿’åˆ°çš„é‡è¦æ€§æ¬Šé‡
            # è§£æ±º"ç‚ºæ¯å€‹æ•¸æ“šé›†çš„é‡è¦è£œä¸æ¨™è¨˜è³¦äºˆæ›´é«˜æ¬Šé‡"çš„éœ€æ±‚
            weighted_scale = scale_output * importance_weights[i]
            weighted_scales.append(weighted_scale)
            start_idx = end_idx
        
        # é€£æ¥åŠ æ¬Šå¾Œçš„å°ºåº¦ç‰¹å¾µ
        final_output = torch.cat(weighted_scales, dim=1)
        
        # æœ€çµ‚æŠ•å½±
        final_output = self.output_projection(final_output)
        
        # é‡å¡‘ç‚ºæ¨™æº–è¼¸å‡ºæ ¼å¼
        total_patches = final_output.shape[1]
        final_output = final_output.view(batch_size, n_vars, total_patches, self.d_model)
        
        return final_output
```

---

## âš™ï¸ èåˆåŸ·è¡Œæ©Ÿåˆ¶è©³è§£

### å¾å›ºå®šåˆ°è‡ªé©æ‡‰çš„è½‰è®Š

**å‚³çµ±æ–¹æ³•çš„å•é¡Œ**ï¼š
```python
# å›ºå®šè£œä¸é•·åº¦æ–¹æ³•çš„é™åˆ¶
patch_length = 16  # å›ºå®šå€¼ï¼Œç„¡æ³•é©æ‡‰ä¸åŒæ•¸æ“šé›†
patches = x.unfold(size=patch_length, step=patch_length)
# çµæœï¼šå¤§éƒ¨åˆ†æ•¸æ“šé›†æ€§èƒ½å—é™
```

**æˆ‘å€‘çš„å‰µæ–°è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# å¤šå°ºåº¦è‡ªé©æ‡‰æ–¹æ³•
patch_sizes = [4, 8, 16, 24, 32]  # å¤šå°ºåº¦è¦†è“‹
scale_embeddings = []
for patch_size in patch_sizes:
    patches = x.unfold(size=patch_size, step=patch_size)
    embedded = self.patch_embeddings[str(patch_size)](patches)
    scale_embeddings.append(embedded)

# æ³¨æ„åŠ›æ©Ÿåˆ¶è‡ªå‹•å­¸ç¿’æœ€ä½³çµ„åˆ
fused = self.attention_fusion(scale_embeddings)
# çµæœï¼šæ¯å€‹æ•¸æ“šé›†éƒ½èƒ½æ‰¾åˆ°æœ€ä½³çš„å°ºåº¦çµ„åˆ
```

---

## ğŸ“Š æ–¹æ³•æ¯”è¼ƒåˆ†æ

### å›ºå®šå°ºåº¦ vs å¤šå°ºåº¦æ–¹æ³•å°æ¯”

| æ–¹æ³•ç‰¹æ€§ | å›ºå®šè£œä¸é•·åº¦ | å¤šå°ºåº¦TimeXer | æ”¹é€²æ•ˆæœ |
|---------|-------------|---------------|----------|
| **é©æ‡‰æ€§** | âŒ å–®ä¸€å›ºå®šå€¼16 | âœ… å¤šå°ºåº¦è‡ªé©æ‡‰ | ğŸš€ é¡¯è‘—æå‡ |
| **æ•¸æ“šè¦†è“‹** | âŒ å±€é™æ–¼ä¸€ç¨®ç²’åº¦ | âœ… å…¨å°ºåº¦è¦†è“‹ | ğŸ“ˆ å®Œæ•´æ€§å¤§å¹…æå‡ |
| **æ¬Šé‡å­¸ç¿’** | âŒ ç„¡æ³•èª¿æ•´ | âœ… æ³¨æ„åŠ›é©…å‹•å­¸ç¿’ | ğŸ¯ æ™ºèƒ½åŒ–æ¬Šé‡åˆ†é… |
| **æ€§èƒ½è¡¨ç¾** | âŒ å¤§éƒ¨åˆ†æ•¸æ“šé›†å—é™ | âœ… å„æ•¸æ“šé›†æœ€ä½³åŒ– | â­ å…¨é¢æ€§èƒ½æå‡ |

### å¯¦é©—é©—è­‰çµæœ

**ç™¼ç¾1ï¼šå›ºå®šè£œä¸é•·åº¦çš„ç“¶é ¸**
- æ¸¬è©¦è£œä¸é•·åº¦ï¼š4, 8, 16, 24, 32
- çµæœï¼šå¤§éƒ¨åˆ†æ•¸æ“šé›†åœ¨16æ™‚æœ€ä½³ï¼Œä½†ä»æœ‰æ”¹é€²ç©ºé–“

**ç™¼ç¾2ï¼šå¤šå°ºåº¦æ–¹æ³•çš„å„ªå‹¢**
- åŒæ™‚ä½¿ç”¨å¤šå€‹è£œä¸é•·åº¦
- æ³¨æ„åŠ›æ©Ÿåˆ¶è‡ªå‹•æ¬Šé‡åˆ†é…
- çµæœï¼šæ¯å€‹æ•¸æ“šé›†éƒ½èƒ½æ‰¾åˆ°æœ€é©åˆçš„å°ºåº¦çµ„åˆ

---

## ğŸ¯ æ–¹æ³•ç¸½çµ

### æ ¸å¿ƒçªç ´

1. **ç†è«–çªç ´**ï¼š
   - ç™¼ç¾å›ºå®šè£œä¸é•·åº¦çš„æ ¹æœ¬é™åˆ¶
   - æå‡ºå¤šå°ºåº¦æ³¨æ„åŠ›èåˆè§£æ±ºæ–¹æ¡ˆ

2. **æŠ€è¡“å‰µæ–°**ï¼š
   - å¤šå°ºåº¦è£œä¸åµŒå…¥æ¶æ§‹
   - æ³¨æ„åŠ›é©…å‹•çš„æ™ºèƒ½æ¬Šé‡å­¸ç¿’
   - æ•¸æ“šè‡ªé©æ‡‰çš„å°ºåº¦çµ„åˆ

3. **å¯¦ç”¨åƒ¹å€¼**ï¼š
   - é©æ‡‰ä¸åŒæ•¸æ“šé›†ç‰¹æ€§
   - è‡ªå‹•ç™¼ç¾æœ€ä½³æ™‚é–“å°ºåº¦çµ„åˆ
   - é¡¯è‘—æå‡é æ¸¬æ€§èƒ½

### æ–¹æ³•å„ªå‹¢

- ğŸ” **æ™ºèƒ½åŒ–**ï¼šè‡ªå‹•å­¸ç¿’æ¯å€‹æ•¸æ“šé›†çš„æœ€é‡è¦è£œä¸æ¨™è¨˜
- ğŸ¯ **è‡ªé©æ‡‰**ï¼šæ ¹æ“šæ•¸æ“šç‰¹æ€§å‹•æ…‹èª¿æ•´å°ºåº¦æ¬Šé‡  
- âš¡ **é«˜æ•ˆæ€§**ï¼šåœ¨æå‡æ€§èƒ½çš„åŒæ™‚ä¿æŒè¨ˆç®—æ•ˆç‡
- ğŸŒ **é€šç”¨æ€§**ï¼šé©ç”¨æ–¼å„ç¨®æ™‚é–“åºåˆ—é æ¸¬ä»»å‹™

### é©ç”¨å ´æ™¯

- ğŸ“ˆ **å¤šè®Šæ€§æ•¸æ“š**ï¼šéœ€è¦ä¸åŒæ™‚é–“å°ºåº¦çš„è¤‡é›œåºåˆ—
- ğŸ¯ **æ€§èƒ½é—œéµ**ï¼šå°é æ¸¬ç²¾åº¦æœ‰é«˜è¦æ±‚çš„æ‡‰ç”¨
- ğŸ”¬ **ç ”ç©¶å‰µæ–°**ï¼šæ¢ç´¢æ™‚é–“åºåˆ—çš„å¤šå°ºåº¦ç‰¹æ€§
- ğŸ­ **å·¥æ¥­æ‡‰ç”¨**ï¼šéœ€è¦é©æ‡‰ä¸åŒæ•¸æ“šç‰¹æ€§çš„å¯¦éš›å ´æ™¯

---

*ğŸ“ å°ˆé¡Œå ±å‘Š | ç ”ç©¶é‡é»ï¼šçªç ´å›ºå®šè£œä¸é•·åº¦é™åˆ¶ï¼Œå¯¦ç¾å¤šå°ºåº¦è‡ªé©æ‡‰èåˆ*
