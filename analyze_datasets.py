import os
import sys
import pandas as pd
import numpy as np
import torch
from data_provider.data_factory import data_provider
import argparse

class DatasetAnalyzer:
    """åˆ†æ TimeXer é …ç›®ä¸­æ‰€æœ‰æ•¸æ“šé›†çš„çµ±è¨ˆè³‡è¨Š"""
    
    def __init__(self):
        self.datasets_info = {
            'ETTh1': {
                'data_path': 'ETTh1.csv',
                'root_path': './dataset/ETT-small/',
                'data_type': 'ETTh1',
                'features': 'M',
                'target': 'OT',
                'enc_in': 7,
                'dec_in': 7,
                'c_out': 7
            },
            'ETTh2': {
                'data_path': 'ETTh2.csv', 
                'root_path': './dataset/ETT-small/',
                'data_type': 'ETTh2',
                'features': 'M',
                'target': 'OT',
                'enc_in': 7,
                'dec_in': 7,
                'c_out': 7
            },
            'ETTm1': {
                'data_path': 'ETTm1.csv',
                'root_path': './dataset/ETT-small/', 
                'data_type': 'ETTm1',
                'features': 'M',
                'target': 'OT',
                'enc_in': 7,
                'dec_in': 7,
                'c_out': 7
            },
            'ETTm2': {
                'data_path': 'ETTm2.csv',
                'root_path': './dataset/ETT-small/',
                'data_type': 'ETTm2', 
                'features': 'M',
                'target': 'OT',
                'enc_in': 7,
                'dec_in': 7,
                'c_out': 7
            },
            'Weather': {
                'data_path': 'weather.csv',
                'root_path': './dataset/weather/',
                'data_type': 'custom',
                'features': 'M',
                'target': 'OT',
                'enc_in': 21,
                'dec_in': 21,
                'c_out': 21
            },
            'Traffic': {
                'data_path': 'traffic.csv',
                'root_path': './dataset/traffic/',
                'data_type': 'custom', 
                'features': 'M',
                'target': 'OT',
                'enc_in': 862,
                'dec_in': 862,
                'c_out': 862
            },
            'ECL': {
                'data_path': 'electricity.csv',
                'root_path': './dataset/electricity/',
                'data_type': 'custom',
                'features': 'M', 
                'target': 'OT',
                'enc_in': 321,
                'dec_in': 321,
                'c_out': 321
            },
            'Exchange': {
                'data_path': 'exchange_rate.csv',
                'root_path': './dataset/exchange_rate/',
                'data_type': 'custom',
                'features': 'M',
                'target': 'OT',
                'enc_in': 8,
                'dec_in': 8,
                'c_out': 8
            }
        }
        
    def create_mock_args(self, dataset_name, dataset_info):
        """å‰µå»ºæ¨¡æ“¬çš„åƒæ•¸å°è±¡ä¾†åˆå§‹åŒ–æ•¸æ“šåŠ è¼‰å™¨"""
        class MockArgs:
            def __init__(self):
                # åŸºæœ¬æ•¸æ“šåƒæ•¸
                self.data = dataset_info['data_type']
                self.root_path = dataset_info['root_path'] 
                self.data_path = dataset_info['data_path']
                self.features = dataset_info['features']
                self.target = dataset_info['target']
                self.freq = 'h' if 'ETT' in dataset_name else 't'
                self.checkpoints = './checkpoints'
                
                # æ¨¡å‹çµæ§‹åƒæ•¸
                self.enc_in = dataset_info['enc_in']
                self.dec_in = dataset_info['dec_in'] 
                self.c_out = dataset_info['c_out']
                
                # TimeXer æ¨™æº–åƒæ•¸
                self.seq_len = 96
                self.label_len = 48
                self.pred_len = 96
                self.seasonal_patterns = None
                self.inverse = False
                self.num_workers = 0
                self.batch_size = 32
                
                # æ™‚é–“ç·¨ç¢¼åƒæ•¸
                self.timeenc = 0
                self.embed = 'timeF'
                
                # ä»»å‹™ç›¸é—œåƒæ•¸
                self.task_name = 'long_term_forecast'
                self.is_training = 1
                self.model_id = f'{dataset_name}_96_96'
                self.model = 'TimeXer'
                
                # æ•¸æ“šå¢å¼·åƒæ•¸
                self.augmentation_ratio = 0
                self.seed = 2021
                self.jitter = False
                self.scaling = False
                self.permutation = False
                self.randompermutation = False
                self.magwarp = False
                self.timewarp = False
                self.windowslice = False
                self.windowwarp = False
                self.rotation = False
                self.spawner = False
                self.dtwwarp = False
                self.shapedtwwarp = False
                self.wdba = False
                self.discdtw = False
                self.discsdtw = False
                self.extra_tag = ""
                
                # å…¶ä»–å¿…è¦åƒæ•¸
                self.scale = True
                self.use_gpu = torch.cuda.is_available()
                self.gpu = 0
                self.use_multi_gpu = False
                self.devices = '0,1,2,3'
                self.detail_freq = self.freq
                
                # TimeXer ç‰¹æœ‰åƒæ•¸
                self.patch_len = 16
                self.stride = 8
                self.gpt_layers = 3
                self.is_gpt = 1
                self.e_layers = 3
                self.d_layers = 1
                self.factor = 1
                self.d_model = 768
                self.n_heads = 4
                self.d_ff = 768
                self.moving_avg = 25
                self.dropout = 0.3
                self.fc_dropout = 0.3
                self.head_dropout = 0.0
                self.des = 'Exp'
                self.train_epochs = 10
                self.patience = 3
                self.learning_rate = 0.0001
                self.lradj = 'type1'
                self.use_amp = False
                
        return MockArgs()
        
    def analyze_raw_data(self, dataset_name, dataset_info):
        """ç›´æ¥è®€å–åŸå§‹æ•¸æ“šæ–‡ä»¶é€²è¡Œåˆ†æ"""
        file_path = os.path.join(dataset_info['root_path'], dataset_info['data_path'])
        
        if not os.path.exists(file_path):
            return None
            
        try:
            df = pd.read_csv(file_path)
            
            # åŸºæœ¬çµ±è¨ˆ
            total_timesteps = len(df)
            total_columns = len(df.columns)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ™‚é–“åˆ—
            time_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
            
            # æ•¸å€¼åˆ—ï¼ˆæ’é™¤æ™‚é–“åˆ—ï¼‰
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # ç§»é™¤æ™‚é–“ç›¸é—œåˆ—
            for time_col in time_columns:
                if time_col in numeric_columns:
                    numeric_columns.remove(time_col)
                    
            num_numeric_features = len(numeric_columns)
            
            # è¨ˆç®—æ•¸æ“šçµ±è¨ˆ
            data_stats = {}
            if len(numeric_columns) > 0:
                numeric_df = df[numeric_columns]
                data_stats = {
                    'mean': numeric_df.mean().mean(),
                    'std': numeric_df.std().mean(),
                    'min': numeric_df.min().min(),
                    'max': numeric_df.max().max(),
                    'missing_values': numeric_df.isnull().sum().sum()
                }
            
            return {
                'total_timesteps': total_timesteps,
                'total_columns': total_columns,
                'time_columns': time_columns,
                'numeric_columns': numeric_columns,
                'num_numeric_features': num_numeric_features,
                'data_stats': data_stats,
                'sample_columns': list(df.columns)[:10]  # å‰10åˆ—åç¨±
            }
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return None
    
    def analyze_timexer_data_split(self, dataset_name, dataset_info):
        """ä½¿ç”¨ TimeXer çš„æ•¸æ“šåŠ è¼‰å™¨åˆ†ææ•¸æ“šåŠƒåˆ†"""
        try:
            args = self.create_mock_args(dataset_name, dataset_info)
            print(f"  ğŸ”§ æ­£åœ¨åˆ†æ {dataset_name} çš„ TimeXer æ•¸æ“šåŠƒåˆ†...")
            
            # ç²å–è¨“ç·´ã€é©—è­‰ã€æ¸¬è©¦æ•¸æ“šé›†
            results = {}
            
            for flag in ['train', 'val', 'test']:
                try:
                    print(f"    â³ åŠ è¼‰ {flag} æ•¸æ“š...")
                    data_set, data_loader = data_provider(args, flag)
                    
                    # ç²å–æ•¸æ“šé›†ä¿¡æ¯
                    dataset_length = len(data_set)
                    print(f"    âœ… {flag} æ•¸æ“šé›†å¤§å°: {dataset_length}")
                    
                    # ç²å–ä¸€å€‹batchä¾†æª¢æŸ¥ç¶­åº¦
                    for batch_idx, batch in enumerate(data_loader):
                        batch_x, batch_y, batch_x_mark, batch_y_mark = batch
                        
                        batch_size = batch_x.shape[0]
                        seq_len = batch_x.shape[1] 
                        n_features = batch_x.shape[2]
                        
                        # å¤–ç”Ÿè®Šæ•¸ç¶­åº¦
                        if batch_x_mark is not None:
                            n_exo_features = batch_x_mark.shape[2]
                        else:
                            n_exo_features = 0
                            
                        results[flag] = {
                            'dataset_length': dataset_length,
                            'batch_size': batch_size,
                            'seq_len': seq_len,
                            'n_endogenous_features': n_features,
                            'n_exogenous_features': n_exo_features,
                            'batch_x_shape': list(batch_x.shape),
                            'batch_y_shape': list(batch_y.shape),
                            'batch_x_mark_shape': list(batch_x_mark.shape) if batch_x_mark is not None else None,
                            'batch_y_mark_shape': list(batch_y_mark.shape) if batch_y_mark is not None else None,
                            'data_range': {
                                'x_min': float(batch_x.min()),
                                'x_max': float(batch_x.max()),
                                'x_mean': float(batch_x.mean())
                            }
                        }
                        print(f"    ğŸ“Š {flag} batch shape: {batch_x.shape}, target shape: {batch_y.shape}")
                        break
                        
                except Exception as e:
                    print(f"    âŒ åŠ è¼‰ {flag} æ•¸æ“šå¤±æ•—: {e}")
                    results[flag] = f"Error: {e}"
                    
            return results
            
        except Exception as e:
            print(f"  âŒ åˆ†æ {dataset_name} å¤±æ•—: {e}")
            return f"Error analyzing {dataset_name}: {e}"
    
    def print_analysis_results(self):
        """æ‰“å°æ‰€æœ‰æ•¸æ“šé›†çš„åˆ†æçµæœ"""
        print("=" * 80)
        print("TimeXer æ•¸æ“šé›†åˆ†æå ±å‘Š")
        print("=" * 80)
        
        summary_table = []
        detailed_info = {}
        
        for dataset_name, dataset_info in self.datasets_info.items():
            print(f"\nğŸ“Š æ•¸æ“šé›†: {dataset_name}")
            print("-" * 60)
            
            # åŸå§‹æ•¸æ“šåˆ†æ
            raw_analysis = self.analyze_raw_data(dataset_name, dataset_info)
            if raw_analysis:
                print(f"åŸå§‹æ•¸æ“š:")
                print(f"  ğŸ“ˆ ç¸½æ™‚é–“æ­¥æ•¸: {raw_analysis['total_timesteps']:,}")
                print(f"  ğŸ“‹ ç¸½åˆ—æ•¸: {raw_analysis['total_columns']}")
                print(f"  ğŸ”¢ æ•¸å€¼ç‰¹å¾µæ•¸: {raw_analysis['num_numeric_features']}")
                print(f"  ğŸ“… æ™‚é–“åˆ—: {raw_analysis['time_columns']}")
                
                if raw_analysis['data_stats']:
                    stats = raw_analysis['data_stats']
                    print(f"  ğŸ“Š æ•¸æ“šçµ±è¨ˆ:")
                    print(f"    å¹³å‡å€¼: {stats['mean']:.3f}")
                    print(f"    æ¨™æº–å·®: {stats['std']:.3f}")
                    print(f"    ç¯„åœ: [{stats['min']:.3f}, {stats['max']:.3f}]")
                    print(f"    ç¼ºå¤±å€¼: {stats['missing_values']}")
                
                # TimeXer æ•¸æ“šåŠƒåˆ†åˆ†æ
                timexer_analysis = self.analyze_timexer_data_split(dataset_name, dataset_info)
                
                if isinstance(timexer_analysis, dict):
                    # ä½¿ç”¨ val æˆ– test çš„ä¿¡æ¯ï¼ˆå› ç‚º train å¯èƒ½å¤±æ•—ï¼‰
                    reference_info = None
                    for split in ['train', 'val', 'test']:
                        if split in timexer_analysis and isinstance(timexer_analysis[split], dict):
                            reference_info = timexer_analysis[split]
                            break
                    
                    if reference_info:
                        print(f"\nTimeXer æ•¸æ“šåŠƒåˆ†:")
                        print(f"  ğŸ¯ å…§ç”Ÿè®Šæ•¸æ•¸é‡: {reference_info['n_endogenous_features']}")
                        print(f"  ğŸŒ å¤–ç”Ÿè®Šæ•¸æ•¸é‡: {reference_info['n_exogenous_features']}")
                        print(f"  ğŸ“Š åºåˆ—é•·åº¦: {reference_info['seq_len']}")
                        print(f"  ğŸ“¦ æ‰¹æ¬¡å¤§å°: {reference_info['batch_size']}")
                        
                        if 'data_range' in reference_info:
                            dr = reference_info['data_range']
                            print(f"  ğŸ“ˆ æ•¸æ“šç¯„åœ: [{dr['x_min']:.3f}, {dr['x_max']:.3f}], å¹³å‡: {dr['x_mean']:.3f}")
                        
                        print(f"\næ•¸æ“šé›†å¤§å°:")
                        train_samples = val_samples = test_samples = "N/A"
                        for split in ['train', 'val', 'test']:
                            if split in timexer_analysis and isinstance(timexer_analysis[split], dict):
                                length = timexer_analysis[split]['dataset_length']
                                print(f"  {split.capitalize()}: {length:,} samples")
                                if split == 'train':
                                    train_samples = f"{length:,}"
                                elif split == 'val':
                                    val_samples = f"{length:,}"
                                elif split == 'test':
                                    test_samples = f"{length:,}"
                            elif split in timexer_analysis:
                                print(f"  {split.capitalize()}: åŠ è¼‰å¤±æ•—")
                                if split == 'train':
                                    train_samples = "Failed"
                        
                        # è¨ˆç®—æ•¸æ“šåŠƒåˆ†æ¯”ä¾‹ï¼ˆå¦‚æœæœ‰è¶³å¤ ä¿¡æ¯ï¼‰
                        valid_splits = [split for split in ['train', 'val', 'test'] 
                                      if split in timexer_analysis and isinstance(timexer_analysis[split], dict)]
                        
                        if len(valid_splits) >= 2:
                            print(f"\nå¯ç”¨æ•¸æ“šåŠƒåˆ†:")
                            for split in valid_splits:
                                length = timexer_analysis[split]['dataset_length']
                                print(f"  {split.capitalize()}: {length:,} samples")
                        
                        # æ·»åŠ åˆ°æ‘˜è¦è¡¨
                        summary_table.append([
                            dataset_name,
                            f"{raw_analysis['total_timesteps']:,}",
                            f"{reference_info['n_endogenous_features']}",
                            f"{reference_info['n_exogenous_features']}",
                            train_samples,
                            val_samples,
                            test_samples
                        ])
                        
                        # ä¿å­˜è©³ç´°ä¿¡æ¯
                        detailed_info[dataset_name] = {
                            'raw': raw_analysis,
                            'timexer': timexer_analysis
                        }
                else:
                    print(f"TimeXer åˆ†æå¤±æ•—: {timexer_analysis}")
                    summary_table.append([dataset_name, f"{raw_analysis['total_timesteps']:,}", "N/A", "N/A", "N/A", "N/A", "N/A"])
            else:
                print(f"âŒ ç„¡æ³•è®€å–æ•¸æ“šé›†æ–‡ä»¶: {dataset_info['root_path']}{dataset_info['data_path']}")
                summary_table.append([dataset_name, "N/A", "N/A", "N/A", "N/A", "N/A", "N/A"])
        
        # æ‰“å°æ‘˜è¦è¡¨æ ¼
        print("\n" + "=" * 120)
        print("ğŸ“‹ æ•¸æ“šé›†æ‘˜è¦è¡¨")
        print("=" * 120)
        
        if summary_table:
            header = ["æ•¸æ“šé›†", "ç¸½æ™‚é–“æ­¥", "å…§ç”Ÿè®Šæ•¸", "å¤–ç”Ÿè®Šæ•¸", "è¨“ç·´æ¨£æœ¬", "é©—è­‰æ¨£æœ¬", "æ¸¬è©¦æ¨£æœ¬"]
            
            # è¨ˆç®—åˆ—å¯¬
            col_widths = [max(len(str(row[i])) for row in [header] + summary_table) + 2 for i in range(len(header))]
            
            # æ‰“å°è¡¨é ­
            header_row = "|".join(f"{header[i]:^{col_widths[i]}}" for i in range(len(header)))
            print(header_row)
            print("-" * len(header_row))
            
            # æ‰“å°æ•¸æ“šè¡Œ
            for row in summary_table:
                data_row = "|".join(f"{row[i]:^{col_widths[i]}}" for i in range(len(row)))
                print(data_row)
        
        # æ‰“å°é¡å¤–çš„æ´å¯Ÿ
        print("\n" + "=" * 80)
        print("ğŸ“ˆ æ•¸æ“šé›†ç‰¹æ€§åˆ†æ")
        print("=" * 80)
        
        # æŒ‰ç‰¹å¾µæ•¸é‡åˆ†é¡
        small_datasets = []   # < 10 features
        medium_datasets = []  # 10-100 features  
        large_datasets = []   # > 100 features
        
        for row in summary_table:
            if len(row) > 2 and row[2] != "N/A" and row[2].isdigit():
                features = int(row[2])
                if features < 10:
                    small_datasets.append((row[0], features))
                elif features <= 100:
                    medium_datasets.append((row[0], features))
                else:
                    large_datasets.append((row[0], features))
        
        print(f"\nğŸ”¢ æŒ‰ç‰¹å¾µæ•¸é‡åˆ†é¡:")
        print(f"  å°è¦æ¨¡ (< 10 ç‰¹å¾µ): {[f'{name} ({feat})' for name, feat in small_datasets]}")
        print(f"  ä¸­è¦æ¨¡ (10-100 ç‰¹å¾µ): {[f'{name} ({feat})' for name, feat in medium_datasets]}")  
        print(f"  å¤§è¦æ¨¡ (> 100 ç‰¹å¾µ): {[f'{name} ({feat})' for name, feat in large_datasets]}")
        
        # æŒ‰æ™‚é–“æ­¥é•·åº¦åˆ†é¡
        short_series = []     # < 20k timesteps
        medium_series = []    # 20k-50k timesteps
        long_series = []      # > 50k timesteps
        
        for row in summary_table:
            if len(row) > 1 and row[1] != "N/A":
                try:
                    timesteps = int(row[1].replace(',', ''))
                    if timesteps < 20000:
                        short_series.append((row[0], timesteps))
                    elif timesteps <= 50000:
                        medium_series.append((row[0], timesteps))
                    else:
                        long_series.append((row[0], timesteps))
                except ValueError:
                    continue
        
        print(f"\nâ° æŒ‰æ™‚é–“åºåˆ—é•·åº¦åˆ†é¡:")
        print(f"  çŸ­åºåˆ— (< 20k æ­¥): {[f'{name} ({ts:,})' for name, ts in short_series]}")
        print(f"  ä¸­ç­‰åºåˆ— (20k-50k æ­¥): {[f'{name} ({ts:,})' for name, ts in medium_series]}")
        print(f"  é•·åºåˆ— (> 50k æ­¥): {[f'{name} ({ts:,})' for name, ts in long_series]}")

        # åˆ†ææ•¸æ“šåŠƒåˆ†æ¨¡å¼
        print(f"\nğŸ“Š TimeXer æ•¸æ“šåŠƒåˆ†æ¨¡å¼:")
        print(f"  - è¼¸å…¥åºåˆ—é•·åº¦: 96 æ™‚é–“æ­¥")
        print(f"  - é æ¸¬åºåˆ—é•·åº¦: 96 æ™‚é–“æ­¥") 
        print(f"  - æ¨™ç±¤åºåˆ—é•·åº¦: 48 æ™‚é–“æ­¥ï¼ˆdecoder è¼¸å…¥ï¼‰")
        print(f"  - å¤–ç”Ÿè®Šæ•¸: æ™‚é–“ç·¨ç¢¼ç‰¹å¾µï¼ˆå¦‚å°æ™‚ã€æ˜ŸæœŸã€æœˆä»½ç­‰ï¼‰")
        
        # æ ¹æ“šæˆåŠŸçš„æ•¸æ“šé›†æä¾›æ´å¯Ÿ
        successful_datasets = [name for name, info in detailed_info.items()]
        if successful_datasets:
            print(f"\nâœ… æˆåŠŸåˆ†æçš„æ•¸æ“šé›†: {successful_datasets}")
            print(f"ğŸ”„ è¨“ç·´æ•¸æ“šåŠ è¼‰å¤±æ•—åŸå› : ç¼ºå°‘æ•¸æ“šå¢å¼·ç›¸é—œåƒæ•¸ï¼ˆå·²ä¿®å¾©ï¼‰")
            
        print("\nğŸ’¡ èªªæ˜:")
        print("- å…§ç”Ÿè®Šæ•¸: æ¨¡å‹è¦é æ¸¬çš„æ™‚é–“åºåˆ—ç‰¹å¾µ")
        print("- å¤–ç”Ÿè®Šæ•¸: æ™‚é–“æ¨™è¨˜ç‰¹å¾µ (å¦‚å°æ™‚ã€æ˜ŸæœŸã€æœˆä»½ç­‰)")
        print("- è¨“ç·´/é©—è­‰/æ¸¬è©¦æ¨£æœ¬: TimeXer æŒ‰æ»‘å‹•çª—å£ç”Ÿæˆçš„æ¨£æœ¬æ•¸")
        print("- TimeXer ä½¿ç”¨ 96 æ™‚é–“æ­¥ä½œç‚ºè¼¸å…¥ï¼Œé æ¸¬ 96 æ™‚é–“æ­¥")
        print("- æ‰¹æ¬¡å½¢ç‹€: [batch_size, seq_len, features]")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ•¸æ“šé›†åˆ†æå™¨...")
    analyzer = DatasetAnalyzer()
    analyzer.print_analysis_results()

if __name__ == "__main__":
    main()
